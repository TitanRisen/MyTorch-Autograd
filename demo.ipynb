{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Tensor import tensor\n",
    "import Tensor\n",
    "from Toolkit import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_e: [[[-2.5, -2.5, -1.66667], [-1.25, -1.0, -0.83333]], [[-0.71429, -0.625, -0.55556], [-0.5, -0.45455, -0.41667]]]\n",
      "grad_f: [[[-0.20833, -0.20833, -0.13889], [-0.10417, -0.08333, -0.06944]], [[-0.05952, -0.05208, -0.0463], [-0.04167, -0.03788, -0.03472]]]\n"
     ]
    }
   ],
   "source": [
    "a=tensor([[[2.,2.,3.],[4., 5., 6.]],[[7.,8.,9.],[10.,11.,12.]]])# dimention: (2,2,3)\n",
    "a.requireGrad(True) \n",
    "b=a**-2\n",
    "c=a**-3\n",
    "d=b*c\n",
    "e=d.log() #dimention (2,2,3)\n",
    "f = e.mean() # scalar\n",
    "print(\"grad_e:\",e.Grad(a))\n",
    "print(\"grad_f:\",f.Grad(a)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 3)\n",
      "(1,)\n",
      "[[2.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0], [10.0, 11.0, 12.0]]\n"
     ]
    }
   ],
   "source": [
    "print(e.shape)\n",
    "print(f.shape)\n",
    "print(a.reshape((4,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_e:  (Variable containing:\n",
      "(0 ,.,.) = \n",
      " -2.5000 -2.5000 -1.6667\n",
      " -1.2500 -1.0000 -0.8333\n",
      "\n",
      "(1 ,.,.) = \n",
      " -0.7143 -0.6250 -0.5556\n",
      " -0.5000 -0.4545 -0.4167\n",
      "[torch.FloatTensor of size 2x2x3]\n",
      ",)\n",
      "\n",
      "grad_f:  \n",
      "(0 ,.,.) = \n",
      " -0.2083 -0.2083 -0.1389\n",
      " -0.1042 -0.0833 -0.0694\n",
      "\n",
      "(1 ,.,.) = \n",
      " -0.0595 -0.0521 -0.0463\n",
      " -0.0417 -0.0379 -0.0347\n",
      "[torch.FloatTensor of size 2x2x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#用pytorch对照\n",
    "a_ = torch.FloatTensor([[[2.,2.,3.],[4., 5., 6.]],[[7.,8.,9.],[10.,11.,12.]]])\n",
    "a_ = torch.autograd.Variable(a_,requires_grad=True)\n",
    "b_=a_**-2\n",
    "c_=a_**-3\n",
    "d_=b_*c_\n",
    "e_=torch.log(d_)\n",
    "_grad = torch.autograd.grad(outputs=e_,inputs=a_,grad_outputs=torch.ones_like(a_))\n",
    "print(\"grad_e: \",_grad)\n",
    "print()\n",
    "f_ = e_.mean()\n",
    "f_.backward()\n",
    "print(\"grad_f: \",a_.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: [-3.7175988006148257, -7.125397701178416, -10.533196601742006]\n"
     ]
    }
   ],
   "source": [
    "# MSE_vector\n",
    "label = tensor([[2.1]])\n",
    "x=tensor([[1.2,2.3,3.4]], require_grad=True)\n",
    "w=tensor([[0.21],[0.53],[-0.3]] ,require_grad=True)\n",
    "w.set_compflow(x) # 和x的计算图相关联，由于外积计算有可能造成计算图的id丢失，所以需要先关联起来\n",
    "b=tensor([0.1] ,require_grad=True)\n",
    "y = x.dot(w) + b \n",
    "# print(y.fullGrad(w)) # 获得全导数矩阵\n",
    "mse = ((y - label) **2).mean()\n",
    "print(\"grad:\", mse.fullGrad(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -3.7176\n",
      " -7.1254\n",
      "-10.5332\n",
      "[torch.FloatTensor of size 3x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_ = torch.FloatTensor([[2.1]])\n",
    "label_ = torch.autograd.Variable(label_,requires_grad=True)\n",
    "x_ = torch.FloatTensor([[1.2,2.3,3.4]])\n",
    "x_ = torch.autograd.Variable(x_,requires_grad=True)\n",
    "w_ = torch.FloatTensor([[0.21],[0.53],[-0.3]])\n",
    "w_ = torch.autograd.Variable(w_,requires_grad=True)\n",
    "b_ = torch.FloatTensor([0.1])\n",
    "b_ = torch.autograd.Variable(b_,requires_grad=False)\n",
    "y_ = torch.mm(x_,w_) + b_\n",
    "_grad = torch.autograd.grad(outputs=y_,inputs=x_,grad_outputs=torch.ones_like(x_))\n",
    "# print(_grad)\n",
    "mse = ((y_ - label_) ** 2).mean()\n",
    "mse.backward()\n",
    "print(w_.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse: [2.1864999999999997]\n",
      "grad result: [-6.869997493901067, -9.809996491426958, -12.749995488952846]\n"
     ]
    }
   ],
   "source": [
    "# MSE_matrix\n",
    "label = tensor([[2.1],[3.1]])\n",
    "x=tensor([[1.,2.,3.],[4., 5., 6.]], require_grad=True)\n",
    "w=tensor([[0.21],[0.53],[-0.3]] ,require_grad=True)\n",
    "w.set_compflow(x) # 和x的计算图相关联，由于外积计算有可能造成计算图的id丢失，所以需要先关联起来\n",
    "b=tensor([0.1] ,require_grad=True)\n",
    "y = x.dot(w) + b \n",
    "# print(y.fullGrad(w))\n",
    "mse = ((y - label) **2).mean()\n",
    "grad_w = mse.fullGrad(w) # 获得全导数张量 ,由于外积计算在trace计算时有问题，不能直接获取偏导数\n",
    "# print(grad_w)\n",
    "# 整合全导数\n",
    "result = [0]*w.shape[0]\n",
    "a,b = grad_w.shape[1],grad_w.shape[2]\n",
    "for i in range(0,a):\n",
    "    for j in range(3):\n",
    "        result[j] += grad_w[i][i][j+i*(b//2)]\n",
    "print(\"mse:\", mse)\n",
    "print(\"grad result:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2.1865\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "\n",
      " -6.8700\n",
      " -9.8100\n",
      "-12.7500\n",
      "[torch.FloatTensor of size 3x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pytorch 0.3.0 , 0.4.0以上合并了Tensor和Variable\n",
    "label_ = torch.FloatTensor([[2.1],[3.1]])\n",
    "label_ = torch.autograd.Variable(label_,requires_grad=True)\n",
    "x_ = torch.FloatTensor([[1.,2.,3.],[4., 5., 6.]])\n",
    "x_ = torch.autograd.Variable(x_,requires_grad=True)\n",
    "w_ = torch.FloatTensor([[0.21],[0.53],[-0.3]])\n",
    "w_ = torch.autograd.Variable(w_,requires_grad=True)\n",
    "b_ = torch.FloatTensor([0.1])\n",
    "b_ = torch.autograd.Variable(b_,requires_grad=False)\n",
    "y_ = torch.mm(x_,w_) + b_\n",
    "_grad = torch.autograd.grad(outputs=y_,inputs=x_,grad_outputs=torch.ones_like(x_))\n",
    "# print(_grad)\n",
    "mse = ((y_ - label_) ** 2).mean()\n",
    "print(mse)\n",
    "mse.backward()\n",
    "print(w_.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
